{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Policy Gradients on CartPole with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Categorical\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env.seed(1); torch.manual_seed(1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradients\n",
    "A policy gradient attempts to train an agent without explicitly mapping the value for every state-action pair in an environment by taking small steps and updating the policy based on the reward associated with that step. The agent can receive a reward immediately for an action or the agent can receive the award at a later time such as the end of the episode. \n",
    "We’ll designate the policy function our agent is trying to learn as $\\pi_\\theta(a,s)$, where $\\theta$ is the parameter vector, $s$ is a particular state, and $a$ is an action.\n",
    "\n",
    "We'll apply a technique called Monte-Carlo Policy Gradient which means we will have the agent run through an entire episode and then update our policy based on the rewards obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Construction\n",
    "### Create Neural Network Model\n",
    "We will use a simple feed forward neural network with one hidden layer of 128 neurons and a dropout of 0.6.  We'll use Adam as our optimizer and a learning rate of 0.01.  Using dropout will significantly improve the performance of our policy.  I encourage you to compare results with and without dropout and experiment with other hyper-parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "learning_rate = 0.01\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.state_space = env.observation_space.shape[0]\n",
    "        self.action_space = env.action_space.n\n",
    "        \n",
    "        self.l1 = nn.Linear(self.state_space, 128, bias=False)\n",
    "        self.l2 = nn.Linear(128, self.action_space, bias=False)\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Episode policy and reward history \n",
    "        self.policy_history = None\n",
    "        self.reward_episode = []\n",
    "        # Overall reward and loss history\n",
    "        self.reward_history = []\n",
    "        self.loss_history = []\n",
    "\n",
    "    def forward(self, x):    \n",
    "        model = torch.nn.Sequential(\n",
    "            self.l1,\n",
    "            nn.Dropout(p=0.6),\n",
    "            nn.ReLU(),\n",
    "            self.l2,\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        return model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = Policy()\n",
    "optimizer = optim.Adam(policy.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Action\n",
    "The select_action function chooses an action based on our policy probability distribution using the PyTorch distributions package.  Our policy returns a probability for each possible action in our action space (move left or move right) as an array of length two such as [0.7, 0.3].  We then choose an action based on these probabilities, record our history, and return our action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    #Select an action (0 or 1) by running policy model and choosing based on the probabilities in state\n",
    "    state = torch.from_numpy(state).type(torch.FloatTensor)\n",
    "    state = policy(Variable(state))\n",
    "    c = Categorical(state)\n",
    "    action = c.sample()\n",
    "    \n",
    "    # Add log probability of our chosen action to our history    \n",
    "    if policy.policy_history == None:\n",
    "        policy.policy_history = torch.tensor([c.log_prob(action)], requires_grad=True)\n",
    "    else:\n",
    "        policy.policy_history = torch.cat([policy.policy_history, torch.tensor([c.log_prob(action)], requires_grad=True)])\n",
    "    return action.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward $v_t$\n",
    "We update our policy by taking a sample of the action value function $Q^{\\pi_\\theta} (s_t,a_t)$ by playing through episodes of the game.  $Q^{\\pi_\\theta} (s_t,a_t)$ is defined as the expected return by taking action $a$ in state $s$ following policy $\\pi$.\n",
    "\n",
    "We know that for every step the simulation continues we receive a reward of 1.  We can use this to calculate the policy gradient at each time step, where $r$ is the reward for a particular state-action pair.  Rather than using the instantaneous reward, $r$, we instead use a long term reward $ v_{t} $ where $v_t$ is the discounted sum of all future rewards for the length of the episode.  In this way, the **longer** the episode runs into the future, the **greater** the reward for a particular state-action pair in the present. $v_{t}$ is then,\n",
    "\n",
    "$$ v_{t} = \\sum_{k=0}^{N} \\gamma^{k}r_{t+k} $$\n",
    "\n",
    "where $\\gamma$ is the discount factor (0.99).  For example, if an episode lasts 5 steps, the reward for each step will be [4.90, 3.94, 2.97, 1.99, 1].\n",
    "Next we scale our reward vector by substracting the mean from each element and scaling to unit variance by dividing by the standard deviation.  This practice is common for machine learning applications and the same operation as Scikit Learn's __[StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)__.  It also has the effect of compensating for future uncertainty.\n",
    "\n",
    "## Update Policy\n",
    "After each episode we apply Monte-Carlo Policy Gradient to improve our policy according to the equation:\n",
    "\n",
    "$$\\Delta\\theta_t = \\alpha\\nabla_\\theta \\, \\log \\pi_\\theta (s_t,a_t)v_t  $$\n",
    "\n",
    "We will then feed our policy history multiplied by our rewards to our optimizer and update the weights of our neural network using stochastic gradent *ascent*.  This should increase the likelihood of actions that got our agent a larger reward.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy():\n",
    "    R = 0\n",
    "    rewards = []\n",
    "    \n",
    "    # Discount future rewards back to the present using gamma\n",
    "    for r in policy.reward_episode[::-1]:\n",
    "        R = r + policy.gamma * R\n",
    "        rewards.insert(0,R)\n",
    "        \n",
    "    # Scale rewards\n",
    "    rewards = torch.FloatTensor(rewards)\n",
    "    rewards = (rewards - rewards.mean()) / (rewards.std() + np.finfo(np.float32).eps)\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss = (torch.sum(torch.mul(policy.policy_history, \n",
    "                                torch.tensor(rewards, requires_grad=True)).mul(-1), -1))\n",
    "    \n",
    "    # Update network weights\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    #Save and intialize episode history counters\n",
    "    policy.loss_history.append(loss.item())\n",
    "    policy.reward_history.append(np.sum(policy.reward_episode))\n",
    "    policy.policy_history = Variable(torch.Tensor())\n",
    "    policy.reward_episode= []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "This is our main policy training loop.  For each step in a training episode, we choose an action, take a step through the environment, and record the resulting new state and reward.  We call update_policy() at the end of each episode to feed the episode history to our neural network and improve our policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(episodes):\n",
    "    running_reward = 10\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset() # Reset environment and record the starting state\n",
    "        done = False       \n",
    "    \n",
    "        for time in range(1000):\n",
    "            action = select_action(state)\n",
    "            # Step through environment using chosen action\n",
    "            state, reward, done, _ = env.step(action)\n",
    "\n",
    "            # Save reward\n",
    "            policy.reward_episode.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Used to determine when the environment is solved.\n",
    "        running_reward = (running_reward * 0.99) + (time * 0.01)\n",
    "\n",
    "        update_policy()\n",
    "\n",
    "        if episode % 50 == 0:\n",
    "            print('Episode {}\\tLast length: {:5d}\\tAverage length: {:.2f}'.format(episode, time, running_reward))\n",
    "\n",
    "        if running_reward > env.spec.reward_threshold:\n",
    "            print(\"Solved! Running reward is now {} and the last episode runs to {} time steps!\".format(running_reward, time))\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\tLast length:    17\tAverage length: 10.07\n",
      "Episode 50\tLast length:    14\tAverage length: 13.55\n",
      "Episode 100\tLast length:    10\tAverage length: 14.86\n",
      "Episode 150\tLast length:    19\tAverage length: 17.03\n",
      "Episode 200\tLast length:    11\tAverage length: 17.89\n",
      "Episode 250\tLast length:    16\tAverage length: 17.87\n",
      "Episode 300\tLast length:    15\tAverage length: 17.82\n",
      "Episode 350\tLast length:    14\tAverage length: 18.32\n",
      "Episode 400\tLast length:    32\tAverage length: 18.85\n",
      "Episode 450\tLast length:    12\tAverage length: 18.30\n",
      "Episode 500\tLast length:    12\tAverage length: 18.12\n",
      "Episode 550\tLast length:    15\tAverage length: 18.48\n",
      "Episode 600\tLast length:    45\tAverage length: 18.67\n",
      "Episode 650\tLast length:    28\tAverage length: 18.75\n",
      "Episode 700\tLast length:    26\tAverage length: 17.97\n",
      "Episode 750\tLast length:    23\tAverage length: 17.53\n",
      "Episode 800\tLast length:    11\tAverage length: 17.65\n",
      "Episode 850\tLast length:    11\tAverage length: 18.87\n",
      "Episode 900\tLast length:     8\tAverage length: 18.42\n",
      "Episode 950\tLast length:    16\tAverage length: 17.64\n"
     ]
    }
   ],
   "source": [
    "episodes = 1000\n",
    "main(episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our policy solves the environment prior to reaching 600 episodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "window = int(episodes/20)\n",
    "\n",
    "fig, ((ax1), (ax2)) = plt.subplots(2, 1, sharey=True, figsize=[9,9]);\n",
    "rolling_mean = pd.Series(policy.reward_history).rolling(window).mean()\n",
    "std = pd.Series(policy.reward_history).rolling(window).std()\n",
    "ax1.plot(rolling_mean)\n",
    "ax1.fill_between(range(len(policy.reward_history)),rolling_mean-std, rolling_mean+std, color='orange', alpha=0.2)\n",
    "ax1.set_title('Episode Length Moving Average ({}-episode window)'.format(window))\n",
    "ax1.set_xlabel('Episode'); ax1.set_ylabel('Episode Length')\n",
    "\n",
    "ax2.plot(policy.reward_history)\n",
    "ax2.set_title('Episode Length')\n",
    "ax2.set_xlabel('Episode'); ax2.set_ylabel('Episode Length')\n",
    "\n",
    "fig.tight_layout(pad=2)\n",
    "plt.show()\n",
    "#fig.savefig('results.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.state_space = env.observation_space.shape[0]\n",
    "        self.action_space = env.action_space.n\n",
    "        \n",
    "        self.l1 = nn.Linear(self.state_space, 128, bias=False)\n",
    "        self.l2 = nn.Linear(128, self.action_space, bias=False)\n",
    "\n",
    "    def forward(self, x):    \n",
    "        model = torch.nn.Sequential(\n",
    "            self.l1,\n",
    "            nn.Dropout(p=0.6),\n",
    "            nn.ReLU(),\n",
    "            self.l2,\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        return model(x)\n",
    "    \n",
    "policy = Policy()\n",
    "optimizer = optim.Adam(policy.parameters(), lr=0.01)\n",
    "\n",
    "def select_action(observation):\n",
    "    #Select an action (0 or 1) by running policy model and choosing based on the probabilities in state\n",
    "    state = torch.tensor(observation, dtype=torch.float)\n",
    "    \n",
    "    probs = policy(state)\n",
    "    # Note that this is equivalent to what used to be called multinomial\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    \n",
    "    return action, m.log_prob(action)\n",
    "\n",
    "def put_reward(action, prob, reward):\n",
    "    loss = -prob * reward / 100\n",
    "    loss.backward()\n",
    "    \n",
    "def update_policy():\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "def main(episodes):\n",
    "    running_reward = 0\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset() # Reset environment and record the starting state\n",
    "    \n",
    "        global_reward = 0\n",
    "        for time in range(1000):\n",
    "            action, prob = select_action(state)\n",
    "            # Step through environment using chosen action\n",
    "            state, reward, done, _ = env.step(action.item())\n",
    "\n",
    "            global_reward += reward\n",
    "            # Save reward\n",
    "            put_reward(action, prob, reward)\n",
    "            env.render()\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Used to determine when the environment is solved.\n",
    "        running_reward = (running_reward + global_reward) / 2\n",
    "\n",
    "        if episode % 1000 == 0:\n",
    "            update_policy()\n",
    "            print('Episode {}\\tLast length: {:5d}\\tAverage length: {:.2f}'.format(episode, time, running_reward))\n",
    "\n",
    "        if running_reward > env.spec.reward_threshold:\n",
    "            print(\"Solved! Running reward is now {} and the last episode runs to {} time steps!\".format(running_reward, time))\n",
    "            break\n",
    "            \n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\tLast length:    17\tAverage length: 9.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "main(episodes = 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": ".env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
